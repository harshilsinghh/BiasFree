{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf3020b-d8dd-447f-9052-88a1a4a1422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import Dataset\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b1f06d-15cd-4a79-a988-f1297b8a3a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17be22b2-55df-4cc0-b9f3-775af77bbca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_PATH = '/kaggle/input/data-under-th4/data_under_th4.csv'\n",
    "MODEL_NAME = 'unitary/toxic-bert'\n",
    "OUTPUT_DIR = './results'\n",
    "SAVE_PATH = './bias_detection_model'\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-5\n",
    "WEIGHT_DECAY = 0.15\n",
    "RANDOM_STATE = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4afc4aa-ccd4-4609-a2d6-c9e2ecbdcdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"Load and preprocess the dataset.\"\"\"\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(file_path, index_col=None)\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    df = df.rename(columns={'0': 'comment_text', '1': 'toxicity'})\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    df = df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "    \n",
    "    # Convert toxicity scores to binary labels (threshold = 0.5)\n",
    "    df['label'] = (df['toxicity'] >= 0.5).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def balance_dataset(train_df):\n",
    "    \"\"\"Balance the training dataset by oversampling biased instances.\"\"\"\n",
    "    # Identify biased samples\n",
    "    biased_df = train_df[train_df['label'] == 1]\n",
    "    \n",
    "    # Concatenate original and duplicated biased data\n",
    "    train_df_balanced = pd.concat([train_df, biased_df])\n",
    "    \n",
    "    return train_df_balanced\n",
    "\n",
    "def compute_class_weights(labels):\n",
    "    \"\"\"Calculate class weights to handle imbalance.\"\"\"\n",
    "    weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=labels)\n",
    "    return torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "def prepare_datasets(train_df, val_df):\n",
    "    \"\"\"Convert DataFrames to Hugging Face Datasets and tokenize.\"\"\"\n",
    "    # Convert to Dataset objects\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset = Dataset.from_pandas(val_df)\n",
    "    \n",
    "    # Tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['comment_text'], \n",
    "                        padding='max_length', \n",
    "                        truncation=True, \n",
    "                        max_length=MAX_LENGTH)\n",
    "    \n",
    "    # Apply tokenization\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Rename 'label' to 'labels' for compatibility\n",
    "    train_dataset = train_dataset.rename_column('label', 'labels')\n",
    "    val_dataset = val_dataset.rename_column('label', 'labels')\n",
    "    \n",
    "    # Set format for PyTorch\n",
    "    train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    \"\"\"Custom Trainer with weighted loss for imbalanced classes.\"\"\"\n",
    "    def compute_loss(self, model, inputs, num_items_in_batch=None, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def __init__(self, class_weights, *args, **kwargs):\n",
    "        self.class_weights = class_weights\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "def setup_training_args():\n",
    "    \"\"\"Configure training arguments with regularization and early stopping.\"\"\"\n",
    "    return TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='eval_loss',\n",
    "        greater_is_better=False,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute evaluation metrics.\"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, training_args, class_weights):\n",
    "    \"\"\"Initialize and train the model.\"\"\"\n",
    "    trainer = WeightedTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        class_weights=class_weights.to(device)\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    return trainer\n",
    "\n",
    "def identify_biased_sentences(text, classifier, threshold=0.3):\n",
    "    \"\"\"Identify biased sentences in a given text.\"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    biased_sentences = []\n",
    "    scores = []\n",
    "    labels = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        result = classifier(sentence)[0]\n",
    "        if result['label'] == 'LABEL_1' and result['score'] > threshold:\n",
    "            biased_sentences.append(sentence)\n",
    "        scores.append(result['score'])\n",
    "        labels.append(result['label'])\n",
    "    \n",
    "    return biased_sentences, scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c03c4f-42bd-4637-a3cd-638dc7c06c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df = load_and_preprocess_data(DATA_PATH)\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Balance the training dataset\n",
    "    train_df_balanced = balance_dataset(train_df)\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weights(train_df_balanced['label'])\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=2,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Prepare datasets\n",
    "    train_dataset, val_dataset = prepare_datasets(train_df_balanced, val_df)\n",
    "    \n",
    "    # Setup training arguments\n",
    "    training_args = setup_training_args()\n",
    "    \n",
    "    # Train the model\n",
    "    trainer = train_model(model, train_dataset, val_dataset, training_args, class_weights)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(\"Validation Metrics:\", eval_results)\n",
    "    \n",
    "    # Save the trained model and tokenizer\n",
    "    model.save_pretrained(SAVE_PATH)\n",
    "    tokenizer.save_pretrained(SAVE_PATH)\n",
    "    \n",
    "    # Load classifier for inference\n",
    "    classifier = pipeline('text-classification', model=SAVE_PATH, tokenizer=SAVE_PATH)\n",
    "    \n",
    "    # Test inference on sample text\n",
    "    sample_text = \"Girls should only do household chores. Sample text. She is a girl, she can't drive.\"\n",
    "    biased_sentences, scores, labels = identify_biased_sentences(sample_text, classifier)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nInference Results:\")\n",
    "    print(\"Biased sentences:\")\n",
    "    for sentence in biased_sentences:\n",
    "        print(f\"- {sentence}\")\n",
    "    print(\"Scores:\")\n",
    "    for score in scores:\n",
    "        print(f\"- {score:.4f}\")\n",
    "    print(\"Labels:\")\n",
    "    for label in labels:\n",
    "        print(f\"- {label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
