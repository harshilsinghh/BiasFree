{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bbaefc-133c-446d-af84-370c139b1df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    roc_auc_score, classification_report\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58065bcd-f121-4da3-95fd-b83d6e86b398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "# Preprocess dataset\n",
    "# Drop the first unnamed column (if exists)\n",
    "data = data.drop(columns=data.columns[0], axis=1)\n",
    "\n",
    "# Rename columns for clarity\n",
    "data = data.rename(columns={'0': 'text', '1': 'label'})\n",
    "\n",
    "# Remove rows with missing values\n",
    "data = data.dropna(subset=['text', 'label'])\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Converts text to lowercase and removes punctuation.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    return text\n",
    "\n",
    "# Apply text preprocessing\n",
    "data['text'] = data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d020e5c2-efcb-4606-a483-a92fe88278be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "glove_path = 'glove.6B.100d.txt'\n",
    "embeddings = {}\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings[word] = vector\n",
    "print(\"GloVe embeddings loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46869a34-b7a1-4069-9a39-840512a4ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text embeddings\n",
    "def get_glove_embedding(text, embeddings, dim=100):\n",
    "    \"\"\"\n",
    "    Converts text into a GloVe-based embedding vector by averaging word embeddings.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    valid_vectors = [embeddings[word] for word in words if word in embeddings]\n",
    "    return np.mean(valid_vectors, axis=0) if valid_vectors else np.zeros(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c42e845-a8b2-4e99-bd55-a6fd80b55cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix\n",
    "X = np.array([get_glove_embedding(text, embeddings) for text in data['text']])\n",
    "y = data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11733953-2c8e-4f41-818c-d227c35e65a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True, stratify=y\n",
    ")\n",
    "\n",
    "# Display class distribution\n",
    "for label, count in enumerate(np.bincount(y)):\n",
    "    print(f\"Class {label}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f338c545-824b-4701-bcb2-40f04021ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train logistic regression model\n",
    "model = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=0.5,\n",
    "    max_iter=100000,\n",
    "    solver='saga'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6fedb5-f156-4cf7-b859-e6af5a112629",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training the model...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88f7ba1-b0d1-4a00-ac03-3e665e8fdf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on training set\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_proba_train = model.predict_proba(X_train)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616e7172-0e13-41ad-98e7-954979cb43ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, y_proba):\n",
    "    \"\"\"\n",
    "    Computes evaluation metrics and prints a classification report.\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"AUC-ROC: {auc:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9be01a-9e1c-4585-89f5-1b6724bfbd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Training Performance ===\")\n",
    "evaluate_model(y_train, y_pred_train, y_pred_proba_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe83bd-959c-4a8d-bb84-21aef49e3830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "print(\"\\n=== Test Performance ===\")\n",
    "evaluate_model(y_test, y_pred, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd225911-5feb-4510-a141-b618ae40fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance at different thresholds\n",
    "def evaluate_at_threshold(y_true, y_proba, threshold):\n",
    "    \"\"\"\n",
    "    Evaluates model performance at a specific probability threshold.\n",
    "    \"\"\"\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    return accuracy_score(y_true, y_pred), precision_score(y_true, y_pred), recall_score(y_true, y_pred), f1_score(y_true, y_pred), roc_auc_score(y_true, y_proba)\n",
    "\n",
    "thresholds = [0.3, 0.5, 0.7]\n",
    "print(\"\\n=== Training Performance at Different Thresholds ===\")\n",
    "for thresh in thresholds:\n",
    "    acc, prec, rec, f1, auc = evaluate_at_threshold(y_train, y_pred_proba_train, thresh)\n",
    "    print(f\"Threshold: {thresh} | Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1:.4f}, AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "print(\"\\n=== Test Performance at Different Thresholds ===\")\n",
    "for thresh in thresholds:\n",
    "    acc, prec, rec, f1, auc = evaluate_at_threshold(y_test, y_pred_proba, thresh)\n",
    "    print(f\"Threshold: {thresh} | Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-Score: {f1:.4f}, AUC-ROC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b8020-9545-4766-8dd0-66e2a3157b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions on new text\n",
    "def predict_gender_bias(text, model, embeddings, dim=100):\n",
    "    \"\"\"\n",
    "    Predicts whether a given text exhibits gender bias using the trained model.\n",
    "    \"\"\"\n",
    "    processed_text = preprocess_text(text)\n",
    "    embedding = get_glove_embedding(processed_text, embeddings, dim).reshape(1, -1)\n",
    "    prediction = model.predict(embedding)[0]\n",
    "    probability = model.predict_proba(embedding)[0][1]\n",
    "    return prediction, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acd11d6-0335-4073-94d8-f7f2cac644ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference\n",
    "test_texts = [\"Test Sentence\"]\n",
    "print(\"\\n=== Inference Examples ===\")\n",
    "for text in test_texts:\n",
    "    pred, prob = predict_gender_bias(text, model, embeddings)\n",
    "    label = \"Biased\" if prob >= 0.3 else \"Unbiased\"\n",
    "    print(f\"Text: '{text}' | Prediction: {label} (Probability of bias: {prob:.4f})\")\n",
    "\n",
    "# Save trained model\n",
    "joblib.dump(model, \"logreg_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
